{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multimodal HCI (Speech and Gesture)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "import time\n",
    "import autopy\n",
    "import threading\n",
    "from __future__ import division\n",
    "import re\n",
    "import sys\n",
    "from google.cloud import speech\n",
    "import pyaudio\n",
    "from six.moves import queue\n",
    "import os\n",
    "import win32api, win32con\n",
    "import keyboard\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speech Commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def mouse_left_click():\n",
    "    win32api.mouse_event(win32con.MOUSEEVENTF_LEFTDOWN,0,0)\n",
    "    time.sleep(0.01)\n",
    "    win32api.mouse_event(win32con.MOUSEEVENTF_LEFTUP,0,0)\n",
    "\n",
    "def mouse_right_click():\n",
    "    win32api.mouse_event(win32con.MOUSEEVENTF_RIGHTDOWN,0,0)\n",
    "    time.sleep(0.01)\n",
    "    win32api.mouse_event(win32con.MOUSEEVENTF_RIGHTUP,0,0)\n",
    "    \n",
    "def scroll(clicks=0, delta_x=0, delta_y=0):\n",
    "    if clicks > 0:\n",
    "        increment = win32con.WHEEL_DELTA\n",
    "    else:\n",
    "        increment = win32con.WHEEL_DELTA * -1\n",
    "\n",
    "    for _ in range(abs(clicks)):\n",
    "        win32api.mouse_event(win32con.MOUSEEVENTF_WHEEL, delta_x, delta_y, increment, 0)\n",
    "        time.sleep(0.05)\n",
    "\n",
    "def copy_item():\n",
    "    mouse_left_click()\n",
    "    keyboard.send(\"ctrl+c\")\n",
    "\n",
    "def cut_item():\n",
    "    mouse_left_click()\n",
    "    keyboard.send(\"ctrl+x\")\n",
    "\n",
    "def paste_item():\n",
    "    mouse_left_click()\n",
    "    keyboard.send(\"ctrl+v\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gesture Recognition System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gesture_recognition():\n",
    "    global stop_threads\n",
    "    w_cam, h_cam = 640, 480 # setting width and height of webcam\n",
    "    w_screen, h_screen = autopy.screen.size() # height and width of screen\n",
    "    frame = 100 # width and height of the frame inside the webcam window\n",
    "    smoothening = 7\n",
    "    prev_loc_x = prev_loc_y = 0\n",
    "    current_loc_x = current_loc_y = 0\n",
    "\n",
    "    cap = cv2.VideoCapture(0) # 0 will take the input from the default camera. 1, 2 etc id's for other cameras\n",
    "    cap.set(3, w_cam) # width (id is 3)\n",
    "    cap.set(4, h_cam) # height (id is 4)\n",
    "    cap.set(10, 100) # brightness (id is 10)\n",
    "\n",
    "    mp_hands = mp.solutions.hands\n",
    "    # hands = mp_hands.Hands() # default parameters are preferred\n",
    "    hands = mp_hands.Hands(False, 1)\n",
    "    mp_draw = mp.solutions.drawing_utils # function to draw (visualize) line and points which was used to detect hands\n",
    "\n",
    "    # used for calculating FPS\n",
    "    previous_time = 0\n",
    "    current_time = 0\n",
    "    count = 0\n",
    "    fps_sum = 0\n",
    "\n",
    "    while True:\n",
    "        count+=1\n",
    "        previous_time = time.time()\n",
    "        success, img = cap.read()\n",
    "        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) # Hands class only uses RGB images\n",
    "        results = hands.process(img_rgb)\n",
    "    #     print(results.multi_hand_landmarks) # will give co-ordinates if hands are detected, else None\n",
    "\n",
    "        if results.multi_hand_landmarks: # if hands are detected\n",
    "#         for hand_lms in results.multi_hand_landmarks: # for each hand detected(max 2 hands by default - Hands())\n",
    "            hand_lms = results.multi_hand_landmarks[0] # first hand (here, only one hand is detected anyway)\n",
    "#         lm - coordinate of point in ratio of h(x),w(y) and z. \n",
    "#         These landmarks will always be in order and hence we get id by enumerate\n",
    "            lm = hand_lms.landmark[9] # total of 21 points(0 - 20 id's)\n",
    "\n",
    "            h, w, c = img.shape # height, width and channels of the img(frame)\n",
    "            cix, ciy = int(lm.x * w), int(lm.y * h) # coordinates with respect to the pixels of the img(frame)           \n",
    "\n",
    "            cv2.circle(img, (cix, ciy), 15, (255,0,255), cv2.FILLED) # highlighting the specific landmark\n",
    "\n",
    "#             frame where hand movement is detected\n",
    "            cv2.rectangle(img, (frame, frame), (w_cam-frame, h_cam-frame), (255, 0, 255), 2)\n",
    "\n",
    "#         csx = np.interp(cix, (0, w_cam), (0, w_screen)) # range from 0 to width of webcam is converted to 0 to width of screen\n",
    "#         csy = np.interp(ciy, (0, h_cam), (0, h_screen)) # range from 0 to height of webcam is converted to 0 to height of screen\n",
    "            csx = np.interp(cix, (frame, w_cam-frame), (0, w_screen)) # range from 0 to width of frame is converted to 0 to width of screen\n",
    "            csy = np.interp(ciy, (frame, h_cam-frame), (0, h_screen)) # range from 0 to height of frame is converted to 0 to height of screen\n",
    "#         print(csx, csy)\n",
    "\n",
    "#         print(\"Landmark: [{0}, {1}]\".format(lm.x, lm.y))\n",
    "#         print(\"Webcam frame coordinates: [{0}, {1}]\".format(cix, ciy))\n",
    "#         print(\"Screen coordinates: [{0}, {1}]\\n\".format(csx, csy))\n",
    "#         Smootheing x and y value\n",
    "            current_loc_x = prev_loc_x + (csx - prev_loc_x) / smoothening\n",
    "            current_loc_y = prev_loc_y + (csy - prev_loc_y) / smoothening\n",
    "\n",
    "#         giving mouse coordinates(x coordinate is inverted, hence we subtract it from width of screen)\n",
    "            autopy.mouse.move(w_screen - current_loc_x, current_loc_y)\n",
    "\n",
    "#         img - destination image, hand_lms - for each hand, mp_hands.HAND_CONNECTIONS - to connect the dots(points)\n",
    "            mp_draw.draw_landmarks(img, hand_lms, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "            prev_loc_x, prev_loc_y = current_loc_x, current_loc_y\n",
    "\n",
    "        current_time = time.time()\n",
    "        fps = 1 / (current_time - previous_time)\n",
    "        fps_sum += fps\n",
    "\n",
    "        cv2.putText(img, str(int(fps)), (10,70), cv2.FONT_HERSHEY_COMPLEX, 2, (0,0,255), 3)\n",
    "        cv2.imshow(\"Video\", img)\n",
    "\n",
    "        if (cv2.waitKey(1) & 0xFF == ord('q')) or stop_threads: \n",
    "#             adds a delay between each image and checks if 'q' is pressed or 'done' is said to close the window\n",
    "            print(\"Average Framerate is:\", fps_sum/count)\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows() # for jupyter notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speech Recognition System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"]=\"SERVICE_KEY.JSON\"\n",
    "\n",
    "# Audio recording parameters\n",
    "RATE = 16000\n",
    "CHUNK = int(RATE / 10)  # 100ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MicrophoneStream(object):\n",
    "    \"\"\"Opens a recording stream as a generator yielding the audio chunks.\"\"\"\n",
    "\n",
    "    def __init__(self, rate, chunk):\n",
    "        self._rate = rate\n",
    "        self._chunk = chunk\n",
    "\n",
    "        # Create a thread-safe buffer of audio data\n",
    "        self._buff = queue.Queue()\n",
    "        self.closed = True\n",
    "\n",
    "    def __enter__(self):\n",
    "        self._audio_interface = pyaudio.PyAudio()\n",
    "        self._audio_stream = self._audio_interface.open(\n",
    "            format=pyaudio.paInt16,\n",
    "            # The API currently only supports 1-channel (mono) audio\n",
    "            # https://goo.gl/z757pE\n",
    "            channels=1,\n",
    "            rate=self._rate,\n",
    "            input=True,\n",
    "            frames_per_buffer=self._chunk,\n",
    "            # Run the audio stream asynchronously to fill the buffer object.\n",
    "            # This is necessary so that the input device's buffer doesn't\n",
    "            # overflow while the calling thread makes network requests, etc.\n",
    "            stream_callback=self._fill_buffer,\n",
    "        )\n",
    "\n",
    "        self.closed = False\n",
    "\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, type, value, traceback):\n",
    "        self._audio_stream.stop_stream()\n",
    "        self._audio_stream.close()\n",
    "        self.closed = True\n",
    "        # Signal the generator to terminate so that the client's\n",
    "        # streaming_recognize method will not block the process termination.\n",
    "        self._buff.put(None)\n",
    "        self._audio_interface.terminate()\n",
    "\n",
    "    def _fill_buffer(self, in_data, frame_count, time_info, status_flags):\n",
    "        \"\"\"Continuously collect data from the audio stream, into the buffer.\"\"\"\n",
    "        self._buff.put(in_data)\n",
    "        return None, pyaudio.paContinue\n",
    "\n",
    "    def generator(self):\n",
    "        while not self.closed:\n",
    "            # Use a blocking get() to ensure there's at least one chunk of\n",
    "            # data, and stop iteration if the chunk is None, indicating the\n",
    "            # end of the audio stream.\n",
    "            chunk = self._buff.get()\n",
    "            if chunk is None:\n",
    "                return\n",
    "            data = [chunk]\n",
    "\n",
    "            # Now consume whatever other data's still buffered.\n",
    "            while True:\n",
    "                try:\n",
    "                    chunk = self._buff.get(block=False)\n",
    "                    if chunk is None:\n",
    "                        return\n",
    "                    data.append(chunk)\n",
    "                except queue.Empty:\n",
    "                    break\n",
    "\n",
    "            yield b\"\".join(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def listen_print_loop(responses):\n",
    "    \"\"\"Iterates through server responses and prints them.\n",
    "\n",
    "    The responses passed is a generator that will block until a response\n",
    "    is provided by the server.\n",
    "\n",
    "    Each response may contain multiple results, and each result may contain\n",
    "    multiple alternatives; for details, see https://goo.gl/tjCPAU.  Here we\n",
    "    print only the transcription for the top alternative of the top result.\n",
    "\n",
    "    In this case, responses are provided for interim results as well. If the\n",
    "    response is an interim one, print a line feed at the end of it, to allow\n",
    "    the next result to overwrite it, until the response is a final one. For the\n",
    "    final one, print a newline to preserve the finalized transcription.\n",
    "    \"\"\"\n",
    "    global stop_threads\n",
    "#     num_chars_printed = 0\n",
    "    \n",
    "    for response in responses:\n",
    "        previous_time = time.time()\n",
    "        if not response.results:\n",
    "#             print(\"Latency =\", time.time() - previous_time)\n",
    "            continue\n",
    "\n",
    "        # The `results` list is consecutive. For streaming, we only care about\n",
    "        # the first result being considered, since once it's `is_final`, it\n",
    "        # moves on to considering the next utterance.\n",
    "        result = response.results[0]\n",
    "        if not result.alternatives:\n",
    "#             print(\"Latency =\", time.time() - previous_time)\n",
    "            continue\n",
    "\n",
    "        # Display the transcription of the top alternative.\n",
    "        transcript = result.alternatives[0].transcript\n",
    "\n",
    "        # Display interim results, but with a carriage return at the end of the\n",
    "        # line, so subsequent lines will overwrite them.\n",
    "        #\n",
    "        # If the previous result was longer than this one, we need to print\n",
    "        # some extra spaces to overwrite the previous result\n",
    "        \n",
    "#         overwrite_chars = \" \" * (num_chars_printed - len(transcript))\n",
    "\n",
    "#         if not result.is_final:\n",
    "#             sys.stdout.write(transcript + overwrite_chars + \"\\r\")\n",
    "#             sys.stdout.flush()\n",
    "\n",
    "#             num_chars_printed = len(transcript)\n",
    "\n",
    "        if result.is_final:\n",
    "            transcript = transcript.strip()\n",
    "#             if(transcript == 'double click' or transcript == 'open' or transcript == \"double-click\"):\n",
    "            if re.search(r\"^(double[- ]?click|open)\", transcript, re.I):\n",
    "                mouse_left_click()\n",
    "                mouse_left_click()\n",
    "                print(transcript.title())\n",
    "#                 print(\"Double Click\")\n",
    "                \n",
    "#             elif(transcript == 'click' or transcript == 'tap' or transcript == 'left click' or transcript == 'left-click' or transcript == 'Click'):\n",
    "            elif re.search(r\"^(click|tap|left[- ]?click)\", transcript, re.I):\n",
    "                mouse_left_click()\n",
    "                print(transcript.title())\n",
    "#                 print(\"Left Click\")\n",
    "                \n",
    "            elif(transcript == 'right click' or transcript == 'right-click' or transcript == \"Rightclick\" or transcript == \"Right-Click\" or transcript == \"Right-click\"):\n",
    "#             elif re.search(r\"^right[- ]?click\", transcript, re.I):\n",
    "                mouse_right_click()\n",
    "                print(transcript.title())\n",
    "#                 print(\"Right Click\")\n",
    "                \n",
    "#             elif(transcript == 'copy'):\n",
    "            elif re.search(r\"^copy\", transcript, re.I):\n",
    "                copy_item()\n",
    "                print(transcript.title())\n",
    "#                 print(\"Copy\")\n",
    "            \n",
    "#             elif(transcript == 'cut' or transcript == 'move'):\n",
    "            elif re.search(r\"^(cut|move)\", transcript, re.I):\n",
    "                cut_item()\n",
    "#                 print(transcript.title())\n",
    "                print(\"Cut\")\n",
    "                \n",
    "#             elif(transcript == 'paste'):\n",
    "            elif re.search(r\"^paste\", transcript, re.I):\n",
    "                paste_item()\n",
    "                print(transcript.title())\n",
    "#                 print(\"Paste\")\n",
    "            \n",
    "#             elif(transcript == 'scroll up' or transcript == 'ScrollUp' or transcript == 'scrollup' or transcript == 'scrollUp' or transcript == 'Scrollup' or transcript =='Scroll Up' or transcript == 'scroll-up' or transcript == 'Scroll-Up'):\n",
    "            elif re.search(r\"^scroll[- ]?up\", transcript, re.I):\n",
    "                scroll(4)\n",
    "                print(transcript.title())\n",
    "#                 print(\"Scroll Up\")\n",
    "            \n",
    "#             elif(transcript == 'scroll down' or transcript == 'ScrollDown' or transcript == 'scrolldown' or transcript == 'scrollDown' or transcript == 'Scrolldown' or transcript =='Scroll Down' or transcript == 'scroll-down' or transcript == 'Scroll-Down'):\n",
    "            elif re.search(r\"^scroll[- ]?down\", transcript, re.I):\n",
    "                scroll(-4)\n",
    "                print(transcript.title())\n",
    "#                 print(\"Scroll Down\")\n",
    "                \n",
    "#             elif(transcript == 'zoom in' or transcript == 'ZoomIn' or transcript == \"zoomin\" or transcript == \"Zoomin\" or transcript == \"zoom-in\" or transcript == 'Zoom-In'):\n",
    "            elif re.search(r\"^zoom[- ]?in\", transcript, re.I):\n",
    "                scroll(8)\n",
    "                print(transcript.title())\n",
    "#                 print(\"Zoom In\")\n",
    "            \n",
    "#             elif(transcript == 'zoom out' or transcript == 'ZoomOut' or transcript == 'zoomout' or transcript == \"Zoom Out\" or transcript == \"zoom-out\" or transcript == \"Zoom-Out\"):\n",
    "            elif re.search(r\"^zoom[- ]?out\", transcript, re.I):\n",
    "                scroll(-8)\n",
    "                print(transcript.title())\n",
    "#                 print(\"Zoom Out\")\n",
    "                \n",
    "#             print(transcript + overwrite_chars)\n",
    "\n",
    "            # Exit recognition if any of the transcribed phrases could be\n",
    "            # one of our keywords.\n",
    "#             elif re.search(r\"\\b(exit|quit|close|done)\\b\", transcript, re.I):\n",
    "            elif re.search(r\"^(exit|quit|close|done)\", transcript, re.I):\n",
    "                stop_threads = True\n",
    "                print(\"Exiting..\")\n",
    "                print(\"Latency =\", time.time() - previous_time)\n",
    "                break\n",
    "\n",
    "#             num_chars_printed = 0\n",
    "            \n",
    "            print(\"Latency =\", time.time() - previous_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def speech_recognition():\n",
    "    global stop_threads\n",
    "    stop_threads = False\n",
    "    # See http://g.co/cloud/speech/docs/languages\n",
    "    # for a list of supported languages.\n",
    "    language_code = \"en-US\"  # a BCP-47 language tag\n",
    "    speech_context = speech.SpeechContext(phrases = ['double click','open','double-click','click','tap','left click',\n",
    "                                                        'left-click','right click','right-click',\n",
    "                                                        'copy','cut','move','paste','scroll up',\n",
    "                                                        'scrollup','scroll-up',\n",
    "                                                        'scroll down','scrolldown',\n",
    "                                                        'scroll-down','zoom in','zoomin',\n",
    "                                                        'zoom-in','zoom out','zoomout',\n",
    "                                                        'zoom-out','close','done'])\n",
    "\n",
    "    client = speech.SpeechClient()\n",
    "    config = speech.RecognitionConfig(\n",
    "        encoding=speech.RecognitionConfig.AudioEncoding.LINEAR16,\n",
    "        sample_rate_hertz=RATE,\n",
    "        language_code=language_code,\n",
    "        speech_contexts = [speech_context],\n",
    "    )\n",
    "\n",
    "    streaming_config = speech.StreamingRecognitionConfig(\n",
    "        config=config, interim_results=True\n",
    "    )\n",
    "\n",
    "    with MicrophoneStream(RATE, CHUNK) as stream:\n",
    "        audio_generator = stream.generator()\n",
    "        requests = (\n",
    "            speech.StreamingRecognizeRequest(audio_content=content)\n",
    "            for content in audio_generator\n",
    "        )\n",
    "\n",
    "        responses = client.streaming_recognize(streaming_config, requests)\n",
    "\n",
    "        # Now, put the transcription responses to use.\n",
    "        print(\"up and running\")\n",
    "        listen_print_loop(responses)\n",
    "        #Speed. I am Speed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multithreading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multithreading():\n",
    "    thread_1 = threading.Thread(target = gesture_recognition)\n",
    "    thread_2 = threading.Thread(target = speech_recognition)\n",
    "\n",
    "    thread_1.start()\n",
    "    thread_2.start()\n",
    "    thread_1.join()\n",
    "    thread_2.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multithreading()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
